"""
Test x·ª≠ l√Ω 1 file PDF ƒë∆°n (ƒë√£ c·∫£i thi·ªán x·ª≠ l√Ω ti·∫øng Vi·ªát sau OCR)
"""

import os
import re
import unicodedata
from typing import Dict, List, Any, Tuple, Optional

from document_processor import EnhancedDocumentProcessor

from docx import Document
from docx.shared import Pt
from docx.enum.text import WD_ALIGN_PARAGRAPH
from docx.oxml.ns import qn


# =========================
# 1) VIETNAMESE POSTPROCESS
# =========================

def normalize_nfc(text: str) -> str:
    """Chu·∫©n ho√° Unicode v·ªÅ NFC ƒë·ªÉ tr√°nh r∆°i d·∫•u/combining marks."""
    if not text:
        return text
    return unicodedata.normalize("NFC", text)


def vi_ocr_cleanup(text: str) -> str:
    """
    L√†m s·∫°ch l·ªói OCR th∆∞·ªùng g·∫∑p (nh·∫π nh√†ng, h·∫°n ch·∫ø ph√° ch·ªØ).
    - normalize NFC
    - chu·∫©n ho√° kho·∫£ng tr·∫Øng
    - s·ª≠a d√≠nh d·∫•u c√¢u
    """
    if not text:
        return text

    text = normalize_nfc(text)

    # chu·∫©n ho√° xu·ªëng d√≤ng
    text = text.replace("\r\n", "\n").replace("\r", "\n")

    # kho·∫£ng tr·∫Øng th·ª´a
    text = re.sub(r"[ \t]+", " ", text)

    # d·∫•u c√¢u d√≠nh v√†o t·ª´
    text = re.sub(r"\s+([,.;:!?])", r"\1", text)          # "t·ª´ ," -> "t·ª´,"
    text = re.sub(r"([,.;:!?])([^\s])", r"\1 \2", text)   # ",t·ª´" -> ", t·ª´"

    # nhi·ªÅu d√≤ng tr·ªëng
    text = re.sub(r"\n{3,}", "\n\n", text)

    return text.strip()


def build_full_text_from_sections(sections: List[Dict[str, Any]]) -> str:
    """Gh√©p text t·ª´ sections v√† cleanup."""
    parts = []
    for sec in sections:
        content = sec.get("content", "")
        content = vi_ocr_cleanup(content)
        if content:
            parts.append(content)
    return "\n\n".join(parts).strip()


# =====================================
# 2) SPELL CORRECTION: POSITION FIRST
# =====================================

def apply_corrections_by_position(full_text: str, errors: List[Dict[str, Any]]) -> Tuple[str, int]:
    """
    S·ª≠a theo v·ªã tr√≠ (position) an to√†n h∆°n regex replace.
    Y√™u c·∫ßu: error['position'] l√† index trong full_text, v√† error['word'] kh·ªõp substring.
    """
    if not full_text or not errors:
        return full_text, 0

    # sort gi·∫£m d·∫ßn ƒë·ªÉ kh√¥ng l·ªách index
    errors_sorted = sorted(
        [e for e in errors if isinstance(e, dict) and e.get("position") is not None],
        key=lambda e: e.get("position", -1),
        reverse=True
    )

    text = full_text
    fixed = 0

    for e in errors_sorted:
        pos = e.get("position", None)
        wrong = e.get("word", "")
        suggs = e.get("suggestions", []) or []
        if pos is None or pos < 0 or not wrong or not suggs:
            continue

        correct = suggs[0]
        wrong = normalize_nfc(str(wrong))
        correct = normalize_nfc(str(correct))

        # ki·ªÉm tra substring t·∫°i v·ªã tr√≠ c√≥ ƒë√∫ng "wrong" kh√¥ng
        segment = text[pos:pos + len(wrong)]
        if normalize_nfc(segment) != wrong:
            # n·∫øu kh√¥ng kh·ªõp, b·ªè qua ƒë·ªÉ tr√°nh thay nh·∫ßm
            continue

        text = text[:pos] + correct + text[pos + len(wrong):]
        fixed += 1

    return text, fixed


def apply_corrections_by_regex(full_text: str, errors: List[Dict[str, Any]]) -> Tuple[str, int]:
    """
    Fallback: s·ª≠a b·∫±ng regex word boundary.
    V·ªõi ti·∫øng Vi·ªát: d√πng boundary ki·ªÉu "kh√¥ng ph·∫£i ch·ªØ/s·ªë/_" ƒë·ªÉ ƒë·ª° sai.
    """
    if not full_text or not errors:
        return full_text, 0

    text = full_text
    fixed = 0

    for e in errors:
        wrong = normalize_nfc(str(e.get("word", "") or ""))
        suggs = e.get("suggestions", []) or []
        if not wrong or not suggs:
            continue

        correct = normalize_nfc(str(suggs[0]))

        # boundary cho Unicode (tr√°nh \b h∆°i l·∫° v·ªõi d·∫•u)
        pattern = rf"(?<![\w√Ä-·ªπ]){re.escape(wrong)}(?![\w√Ä-·ªπ])"
        new_text, n = re.subn(pattern, correct, text, flags=re.IGNORECASE)
        if n > 0:
            fixed += n
            text = new_text

    return text, fixed


def auto_correct_spelling(results: Dict[str, Any]) -> Tuple[str, int]:
    """
    T·ª± ƒë·ªông s·ª≠a l·ªói:
    - Gh√©p + cleanup
    - ∆Øu ti√™n s·ª≠a theo position (n·∫øu position map ƒë√∫ng full_text)
    - Fallback regex
    """
    full_text = build_full_text_from_sections(results.get("sections", []))
    errors = results.get("spelling_check", {}).get("errors", []) or []

    # th·ª≠ s·ª≠a theo position tr∆∞·ªõc
    corrected, fixed_pos = apply_corrections_by_position(full_text, errors)

    # n·∫øu kh√¥ng s·ª≠a ƒë∆∞·ª£c g√¨ (ho·∫∑c qu√° √≠t), fallback regex
    if fixed_pos == 0 and errors:
        corrected, fixed_rx = apply_corrections_by_regex(full_text, errors)
        return corrected, fixed_rx

    return corrected, fixed_pos


# =========================
# 3) WORD EXPORT VI SAFE
# =========================

def set_word_font_vi_safe(doc: Document, font_name: str = "Times New Roman", font_size_pt: int = 13) -> None:
    """
    Set font ƒë·∫ßy ƒë·ªß ƒë·ªÉ Word kh√¥ng fallback g√¢y l·ªói k√Ω t·ª± ti·∫øng Vi·ªát.
    """
    style = doc.styles["Normal"]
    style.font.name = font_name
    style.font.size = Pt(font_size_pt)

    rFonts = style._element.rPr.rFonts
    rFonts.set(qn("w:ascii"), font_name)
    rFonts.set(qn("w:hAnsi"), font_name)
    rFonts.set(qn("w:eastAsia"), font_name)
    rFonts.set(qn("w:cs"), font_name)


def export_corrected_word(corrected_text: str, output_dir: str, filename: str, results: Dict[str, Any], fixed_count: int) -> str:
    """Xu·∫•t vƒÉn b·∫£n ƒë√£ s·ª≠a ra Word (an to√†n ti·∫øng Vi·ªát)."""
    doc = Document()
    set_word_font_vi_safe(doc, font_name="Times New Roman", font_size_pt=13)

    # Ti√™u ƒë·ªÅ
    title = doc.add_heading("T√ÄI LI·ªÜU ƒê√É H·∫¨U X·ª¨ L√ù (TI·∫æNG VI·ªÜT)", 0)
    title.alignment = WD_ALIGN_PARAGRAPH.CENTER

    # Th√¥ng tin
    doc.add_heading("Th√¥ng tin", 1)
    info_text = (
        f"S·ªë l·ªói ƒë√£ s·ª≠a (∆∞·ªõc t√≠nh): {fixed_count}\n"
        f"T·ªïng s·ªë t·ª´: {results.get('metadata', {}).get('total_words', 'N/A')}\n"
        f"T·ª∑ l·ªá l·ªói ban ƒë·∫ßu: {results.get('spelling_check', {}).get('error_rate', 0):.2%}\n"
        f"G·ª£i √Ω: N·∫øu c√≤n r·ª•ng ch·ªØ n·∫∑ng (vd: 'th', 'phn'), h√£y ch·ªânh OCR lang/model trong document_processor.py.\n"
    )
    doc.add_paragraph(info_text)

    # N·ªôi dung
    doc.add_heading("N·ªôi dung", 1)

    # Chia ƒëo·∫°n theo double newline
    paragraphs = corrected_text.split("\n\n")
    for para in paragraphs:
        para = para.strip()
        if para:
            p = doc.add_paragraph(para)
            p.alignment = WD_ALIGN_PARAGRAPH.JUSTIFY

    output_path = os.path.join(output_dir, filename)
    doc.save(output_path)
    return output_path


# =========================
# 4) MAIN
# =========================

def main():
    print("=" * 80)
    print("üß™ TEST X·ª¨ L√ù 1 FILE PDF (c√≥ h·∫≠u x·ª≠ l√Ω ti·∫øng Vi·ªát)")
    print("=" * 80)

    pdf_file = "input_pdfs/ban_thao_van_dap.pdf"  # File nh·ªè nh·∫•t - 83 trang

    if not os.path.exists(pdf_file):
        print(f"‚ùå Kh√¥ng t√¨m th·∫•y file: {pdf_file}")
        print("\nüìù Vui l√≤ng:")
        print("1. Ki·ªÉm tra t√™n file ƒë√∫ng ch∆∞a")
        print("2. Ki·ªÉm tra file c√≥ trong th∆∞ m·ª•c input_pdfs/ kh√¥ng")
        return

    print(f"\nüìÑ File test: {os.path.basename(pdf_file)}")

    output_dir = "output_test_single"
    os.makedirs(output_dir, exist_ok=True)
    print(f"üìÅ K·∫øt qu·∫£ s·∫Ω l∆∞u trong: {output_dir}/")

    confirm = input("\n‚ñ∂Ô∏è  B·∫Øt ƒë·∫ßu x·ª≠ l√Ω? (y/n): ")
    if confirm.lower() != "y":
        print("‚ùå ƒê√£ h·ªßy")
        return

    try:
        processor = EnhancedDocumentProcessor(output_dir=output_dir)

        print("\n‚è≥ ƒêang x·ª≠ l√Ω...")
        results = processor.process_pdf(pdf_file, debug=False)

        # Normalize + cleanup sections ngay sau OCR (quan tr·ªçng)
        for sec in results.get("sections", []):
            sec["content"] = vi_ocr_cleanup(sec.get("content", ""))

        output_filename = f"{os.path.basename(pdf_file).replace('.pdf', '')}_processed.docx"
        output_path = processor.export_to_word(output_filename)

        print("\n" + "=" * 80)
        print("üîß H·∫¨U X·ª¨ L√ù + S·ª¨A (N·∫æU C√ì) L·ªñI CH√çNH T·∫¢")
        print("=" * 80)

        errors = results.get("spelling_check", {}).get("errors", []) or []
        if errors:
            print(f"\nüìù T√¨m th·∫•y {len(errors)} l·ªói (theo spell checker hi·ªán t·∫°i)")
            fix_spelling = input("\n‚ñ∂Ô∏è  B·∫°n c√≥ mu·ªën t·ª± ƒë·ªông s·ª≠a + l√†m s·∫°ch ti·∫øng Vi·ªát? (y/n): ")

            if fix_spelling.lower() == "y":
                corrected_text, fixed_count = auto_correct_spelling(results)

                corrected_file = os.path.join(output_dir, "ocr_result_corrected.txt")
                with open(corrected_file, "w", encoding="utf-8") as f:
                    f.write("=" * 80 + "\n")
                    f.write(f"K·∫æT QU·∫¢ OCR ƒê√É H·∫¨U X·ª¨ L√ù: {os.path.basename(pdf_file)}\n")
                    f.write("=" * 80 + "\n\n")
                    f.write(corrected_text)

                print(f"\n‚úÖ ƒê√£ s·ª≠a (∆∞·ªõc t√≠nh): {fixed_count} thay th·∫ø")
                print(f"üìÑ File ƒë√£ s·ª≠a: {corrected_file}")

                corrected_word_file = f"{os.path.basename(pdf_file).replace('.pdf', '')}_corrected.docx"
                corrected_word_path = export_corrected_word(
                    corrected_text, output_dir, corrected_word_file, results, fixed_count
                )
                print(f"üíæ File Word ƒë√£ s·ª≠a: {corrected_word_path}")
            else:
                print("‚è≠Ô∏è  B·ªè qua h·∫≠u x·ª≠ l√Ω")
        else:
            print("\n‚úÖ Spell checker b√°o: Kh√¥ng c√≥ l·ªói ch√≠nh t·∫£!")

        print("\n" + "=" * 80)
        print("‚úÖ HO√ÄN TH√ÄNH!")
        print("=" * 80)

        md = results.get("metadata", {})
        print(f"\nüìä Th·ªëng k√™:")
        print(f"   üìÑ T·ªïng s·ªë trang: {md.get('total_pages', 'N/A')}")
        print(f"   üìù T·ªïng s·ªë ph·∫ßn: {md.get('total_sections', 'N/A')}")
        print(f"   üñºÔ∏è  T·ªïng s·ªë h√¨nh: {md.get('total_images', 'N/A')}")
        print(f"   üìñ T·ªïng s·ªë t·ª´: {md.get('total_words', 'N/A')}")
        print(f"   ‚ùå L·ªói ch√≠nh t·∫£: {md.get('spelling_errors', 'N/A')}")

        print(f"\nüìù PREVIEW N·ªòI DUNG OCR (5 ph·∫ßn ƒë·∫ßu):")
        print("-" * 80)
        for i, section in enumerate(results.get("sections", [])[:5], 1):
            content_preview = (section.get("content", "")[:200]).replace("\n", " ")
            print(f"\n{i}. Trang {section.get('page', 'N/A')}:")
            print(f"   {content_preview}...")

        print(f"\nüñºÔ∏è  DANH S√ÅCH H√åNH ·∫¢NH ({len(results.get('images', []))} ·∫£nh):")
        print("-" * 80)
        for i, img in enumerate(results.get("images", [])[:10], 1):
            print(f"{i}. {img.get('filename')}")
            print(
                f"   Trang: {img.get('page')} | Format: {img.get('format')} | Path: {img.get('path')}"
            )

        if len(results.get("images", [])) > 10:
            print(f"   ... v√† {len(results.get('images', [])) - 10} h√¨nh ·∫£nh kh√°c")

        # L∆∞u OCR text
        ocr_text_file = os.path.join(output_dir, "ocr_result.txt")
        with open(ocr_text_file, "w", encoding="utf-8") as f:
            f.write("=" * 80 + "\n")
            f.write(f"K·∫æT QU·∫¢ OCR: {os.path.basename(pdf_file)}\n")
            f.write("=" * 80 + "\n\n")

            for section in results.get("sections", []):
                f.write(f"\n{'='*60}\n")
                f.write(f"Trang {section.get('page')} - Section {section.get('id')}\n")
                f.write(f"{'='*60}\n")
                f.write(section.get("content", ""))
                f.write("\n\n")

        print(f"\nüìÑ File text OCR: {ocr_text_file}")

        # L∆∞u danh s√°ch h√¨nh
        images_list_file = os.path.join(output_dir, "images_list.txt")
        with open(images_list_file, "w", encoding="utf-8") as f:
            f.write("=" * 80 + "\n")
            f.write(f"DANH S√ÅCH H√åNH ·∫¢NH: {os.path.basename(pdf_file)}\n")
            f.write(f"T·ªïng s·ªë: {len(results.get('images', []))} h√¨nh ·∫£nh\n")
            f.write("=" * 80 + "\n\n")

            for i, img in enumerate(results.get("images", []), 1):
                f.write(f"{i}. {img.get('filename')}\n")
                f.write(f"   ID: {img.get('id')}\n")
                f.write(f"   Trang: {img.get('page')}\n")
                f.write(f"   Format: {img.get('format')}\n")
                f.write(f"   Path: {img.get('path')}\n")
                bbox = img.get("bbox", {}) or {}
                f.write(
                    f"   Bbox: x={bbox.get('x')}, y={bbox.get('y')}, "
                    f"w={bbox.get('width')}, h={bbox.get('height')}\n"
                )
                f.write("\n")

        print(f"üñºÔ∏è  File danh s√°ch ·∫£nh: {images_list_file}")

        # Stats
        stats_file = os.path.join(output_dir, "statistics.txt")
        with open(stats_file, "w", encoding="utf-8") as f:
            f.write("=" * 80 + "\n")
            f.write(f"TH·ªêNG K√ä CHI TI·∫æT: {os.path.basename(pdf_file)}\n")
            f.write("=" * 80 + "\n\n")

            f.write("T·ªîNG QUAN:\n")
            f.write(f"  T·ªïng s·ªë trang: {md.get('total_pages', 'N/A')}\n")
            f.write(f"  T·ªïng s·ªë ph·∫ßn: {md.get('total_sections', 'N/A')}\n")
            f.write(f"  T·ªïng s·ªë h√¨nh ·∫£nh: {md.get('total_images', 'N/A')}\n")
            f.write(f"  T·ªïng s·ªë t·ª´: {md.get('total_words', 'N/A')}\n")
            f.write(f"  L·ªói ch√≠nh t·∫£: {md.get('spelling_errors', 'N/A')}\n")
            f.write(f"  T·ª∑ l·ªá l·ªói: {results.get('spelling_check', {}).get('error_rate', 0):.2%}\n\n")

            if errors:
                f.write("L·ªñI CH√çNH T·∫¢ (20 l·ªói ƒë·∫ßu):\n")
                for i, error in enumerate(errors[:20], 1):
                    f.write(f"  {i}. '{error.get('word')}' (v·ªã tr√≠: {error.get('position')})\n")
                    if error.get("suggestions"):
                        f.write(f"     G·ª£i √Ω: {', '.join(error.get('suggestions'))}\n")

        print(f"üìä File th·ªëng k√™: {stats_file}")

        print(f"\nüíæ File Word (raw export): {output_path}")
        print(f"üìÅ Th∆∞ m·ª•c h√¨nh ·∫£nh: {output_dir}/images/")
        print("=" * 80)

    except Exception as e:
        print(f"\n‚ùå L·ªñI: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()
