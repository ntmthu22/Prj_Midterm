"""
Enhanced Document Processor vá»›i OCR cáº£i tiáº¿n (FIX tiáº¿ng Viá»‡t bá»‹ rá»¥ng chá»¯)
- Render PDF á»Ÿ DPI cao + sharpen nháº¹
- Preprocessing (denoise + contrast + adaptive threshold) + deskew
- PaddleOCR gá»i báº±ng ocr.ocr(...) á»•n Ä‘á»‹nh cho scan
- Normalize Unicode NFC Ä‘á»ƒ trÃ¡nh rÆ¡i dáº¥u
- Export Word set font Ä‘áº§y Ä‘á»§ Ä‘á»ƒ khÃ´ng máº¥t tiáº¿ng Viá»‡t
"""

import os
import re
import cv2
import fitz
import numpy as np
import unicodedata
from PIL import Image, ImageEnhance

from paddleocr import PaddleOCR
from docx import Document
from docx.shared import Inches, Pt
from docx.enum.text import WD_ALIGN_PARAGRAPH
from docx.oxml.ns import qn

from typing import List, Dict, Tuple, Any


# =========================
# Utils: Unicode
# =========================
def nfc(text: str) -> str:
    return unicodedata.normalize("NFC", text) if text else text


def vi_cleanup(text: str) -> str:
    """Cleanup nháº¹ nhÃ ng sau OCR cho tiáº¿ng Viá»‡t."""
    if not text:
        return text
    text = nfc(text)
    text = text.replace("\r\n", "\n").replace("\r", "\n")
    text = re.sub(r"[ \t]+", " ", text)
    text = re.sub(r"\s+([,.;:!?])", r"\1", text)
    text = re.sub(r"([,.;:!?])([^\s])", r"\1 \2", text)
    text = re.sub(r"\n{3,}", "\n\n", text)
    return text.strip()


# =========================
# Image Preprocessor
# =========================
class ImagePreprocessor:
    """Tiá»n xá»­ lÃ½ hÃ¬nh áº£nh trÆ°á»›c khi OCR"""

    @staticmethod
    def enhance_image(image: np.ndarray) -> np.ndarray:
        """Cáº£i thiá»‡n cháº¥t lÆ°á»£ng hÃ¬nh áº£nh cho OCR."""
        if image is None:
            return image

        # grayscale
        if len(image.shape) == 3:
            gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)
        else:
            gray = image

        # denoise
        denoised = cv2.fastNlMeansDenoising(gray, None, 12, 7, 21)

        # contrast (CLAHE)
        clahe = cv2.createCLAHE(clipLimit=2.2, tileGridSize=(8, 8))
        contrast = clahe.apply(denoised)

        # adaptive threshold
        binary = cv2.adaptiveThreshold(
            contrast, 255,
            cv2.ADAPTIVE_THRESH_GAUSSIAN_C,
            cv2.THRESH_BINARY,
            31, 10
        )

        # morph close (nháº¹)
        kernel = np.ones((1, 1), np.uint8)
        cleaned = cv2.morphologyEx(binary, cv2.MORPH_CLOSE, kernel)

        return cleaned

    @staticmethod
    def deskew_image(image: np.ndarray) -> np.ndarray:
        """Xoay tháº³ng hÃ¬nh áº£nh bá»‹ nghiÃªng."""
        try:
            if image is None:
                return image

            # cáº§n binary Ä‘á»ƒ tÃ¬m gÃ³c
            img = image
            if len(img.shape) == 3:
                img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)

            coords = np.column_stack(np.where(img < 255))  # chá»¯ thÆ°á»ng lÃ  Ä‘iá»ƒm tá»‘i
            if coords.shape[0] < 50:
                return image

            angle = cv2.minAreaRect(coords)[-1]
            if angle < -45:
                angle = -(90 + angle)
            else:
                angle = -angle

            if abs(angle) < 0.3:
                return image

            (h, w) = image.shape[:2]
            center = (w // 2, h // 2)
            M = cv2.getRotationMatrix2D(center, angle, 1.0)
            rotated = cv2.warpAffine(
                image, M, (w, h),
                flags=cv2.INTER_CUBIC,
                borderMode=cv2.BORDER_REPLICATE
            )
            return rotated
        except Exception:
            return image

    @staticmethod
    def resize_for_ocr(image: np.ndarray, target_height: int = 2200) -> np.ndarray:
        """Resize image vá» kÃ­ch thÆ°á»›c tá»‘i Æ°u cho OCR."""
        if image is None:
            return image

        h, w = image.shape[:2]
        if h <= 0 or w <= 0:
            return image

        if h < target_height:
            scale = target_height / h
            new_w = int(w * scale)
            new_h = target_height
            return cv2.resize(image, (new_w, new_h), interpolation=cv2.INTER_CUBIC)

        if h > target_height * 1.6:
            scale = target_height / h
            new_w = int(w * scale)
            new_h = target_height
            return cv2.resize(image, (new_w, new_h), interpolation=cv2.INTER_AREA)

        return image


# =========================
# Vietnamese Spell Checker (fix position = char offset)
# =========================
class VietnameseSpellChecker:
    """Kiá»ƒm tra chÃ­nh táº£ tiáº¿ng Viá»‡t (Ä‘Æ¡n giáº£n)."""

    def __init__(self):
        self.dictionary = set([
            'vÃ ', 'cá»§a', 'cÃ³', 'trong', 'lÃ ', 'Ä‘Æ°á»£c', 'vá»›i', 'cÃ¡c', 'cho', 'Ä‘á»ƒ',
            'Ä‘Ã£', 'nÃ y', 'theo', 'nhá»¯ng', 'ngÆ°á»i', 'tá»«', 'má»™t', 'nÄƒm', 'khi', 'vá»',
            'ná»™i', 'dung', 'hÃ¬nh', 'áº£nh', 'Ä‘á»“', 'thá»‹', 'tÃ i', 'liá»‡u', 'chÆ°Æ¡ng', 'má»¥c',
            'pháº§n', 'báº£ng', 'sá»‘', 'liá»‡u', 'thÃ´ng', 'tin', 'dá»¯', 'kiá»ƒm', 'tra', 'xá»­', 'lÃ½',
            'phÆ°Æ¡ng', 'phÃ¡p', 'káº¿t', 'quáº£', 'nghiÃªn', 'cá»©u', 'phÃ¡t', 'triá»ƒn', 'há»‡', 'thá»‘ng',
        ])
        self.load_extended_dictionary()

    def load_extended_dictionary(self, dict_file='vietnamese_dict.txt'):
        if os.path.exists(dict_file):
            with open(dict_file, 'r', encoding='utf-8') as f:
                words = f.read().splitlines()
                self.dictionary.update([w.strip().lower() for w in words if w.strip()])

    def check_text(self, text: str) -> Dict[str, Any]:
        text = nfc(text or "")
        # dÃ¹ng finditer Ä‘á»ƒ láº¥y luÃ´n char offset
        pattern = re.compile(
            r'[\wÃ Ã¡áº¡áº£Ã£Ã¢áº§áº¥áº­áº©áº«Äƒáº±áº¯áº·áº³áºµÃ¨Ã©áº¹áº»áº½Ãªá»áº¿á»‡á»ƒá»…Ã¬Ã­á»‹á»‰Ä©Ã²Ã³á»á»ÃµÃ´á»“á»‘á»™á»•á»—Æ¡á»á»›á»£á»Ÿá»¡Ã¹Ãºá»¥á»§Å©Æ°á»«á»©á»±á»­á»¯á»³Ã½á»µá»·á»¹Ä‘]+',
            re.IGNORECASE
        )

        errors = []
        total = 0

        for m in pattern.finditer(text.lower()):
            total += 1
            word = m.group(0)
            if len(word) > 2 and word not in self.dictionary:
                errors.append({
                    'word': word,
                    'position': m.start(),     # âœ… char offset (Ä‘á»ƒ sá»­a theo vá»‹ trÃ­)
                    'suggestions': []
                })

        return {
            'total_words': total,
            'errors': errors,
            'error_rate': (len(errors) / total) if total > 0 else 0
        }


# =========================
# Main Processor
# =========================
class EnhancedDocumentProcessor:
    """Document processor vá»›i OCR cáº£i tiáº¿n"""

    def __init__(self, output_dir='output'):
        print("ğŸ”§ Äang khá»Ÿi táº¡o PaddleOCR...")
        print("   (Láº§n Ä‘áº§u sáº½ táº£i model, cÃ³ thá»ƒ máº¥t vÃ i phÃºt...)")

        # âœ… dÃ¹ng API á»•n Ä‘á»‹nh: ocr.ocr(...)
        self.ocr = PaddleOCR(
            lang='vi',
            use_angle_cls=True,
            show_log=False
        )

        self.preprocessor = ImagePreprocessor()
        self.spell_checker = VietnameseSpellChecker()
        self.output_dir = output_dir

        os.makedirs(output_dir, exist_ok=True)
        os.makedirs(f"{output_dir}/images", exist_ok=True)
        os.makedirs(f"{output_dir}/debug", exist_ok=True)

        self.results = {
            'sections': [],
            'images': [],
            'spelling_check': {},
            'metadata': {}
        }

        print("âœ… PaddleOCR sáºµn sÃ ng!\n")

    # ---------
    # OCR Page
    # ---------
    def process_page_image(self, image_rgb: np.ndarray, page_num: int, debug: bool = False) -> List[Dict[str, Any]]:
        """Xá»­ lÃ½ má»™t trang: preprocessing + OCR + sort theo layout."""
        print(f"   ğŸ” Äang OCR trang {page_num}...")

        if image_rgb is None:
            return []

        # 1) resize -> deskew -> enhance (binary)
        img = self.preprocessor.resize_for_ocr(image_rgb, target_height=2200)
        img = self.preprocessor.deskew_image(img)

        # PaddleOCR nháº­n áº£nh RGB/BGR Ä‘á»u Ä‘Æ°á»£c, nhÆ°ng preproc binary lÃ  1 kÃªnh
        bin_img = self.preprocessor.enhance_image(img)

        if debug:
            debug_path = os.path.join(self.output_dir, "debug", f"page_{page_num:03d}_bin.png")
            cv2.imwrite(debug_path, bin_img)

        # 2) OCR
        try:
            ocr_result = self.ocr.ocr(bin_img, cls=True)  # âœ… á»•n Ä‘á»‹nh nháº¥t
        except Exception as e:
            print(f"   âŒ Lá»—i OCR: {e}")
            return []

        if not ocr_result:
            print(f"   âš ï¸  KhÃ´ng phÃ¡t hiá»‡n text trong trang {page_num}")
            return []

        layout_elements: List[Dict[str, Any]] = []
        kept = 0

        # ocr_result: list[line] where line = [box, (text, score)]
        for line in ocr_result:
            try:
                box = line[0]
                text, confidence = line[1][0], float(line[1][1])

                if confidence < 0.50:
                    continue

                text = vi_cleanup(text)
                if not text:
                    continue

                # bbox
                x_coords = [p[0] for p in box]
                y_coords = [p[1] for p in box]
                x_min, x_max = float(min(x_coords)), float(max(x_coords))
                y_min, y_max = float(min(y_coords)), float(max(y_coords))
                width = x_max - x_min
                height = y_max - y_min

                element_type = self.classify_element(text, width, height, y_min)

                layout_elements.append({
                    'type': element_type,
                    'bbox': {
                        'x': int(x_min),
                        'y': int(y_min),
                        'width': int(width),
                        'height': int(height)
                    },
                    'text': text,
                    'confidence': confidence
                })
                kept += 1
            except Exception:
                continue

        # 3) sort top->bottom, left->right (Ä‘á»ƒ ghÃ©p text Ä‘Ãºng thá»© tá»±)
        layout_elements.sort(key=lambda e: (e['bbox']['y'], e['bbox']['x']))

        print(f"   âœ… Giá»¯ {kept} dÃ²ng (confidence > 0.5)")
        return layout_elements

    def classify_element(self, text: str, width: float, height: float, y_pos: float) -> str:
        if height > 32 and y_pos < 220:
            return 'title'
        if re.match(r'^\d+\.\d+', text.strip()):
            return 'heading'
        return 'text'

    # -------------------
    # Extract Images PDF
    # -------------------
    def extract_images_from_pdf(self, pdf_path: str) -> List[Dict[str, Any]]:
        doc = fitz.open(pdf_path)
        images = []

        for page_num in range(len(doc)):
            page = doc[page_num]
            image_list = page.get_images()

            for img_idx, img in enumerate(image_list):
                xref = img[0]
                base_image = doc.extract_image(xref)
                image_bytes = base_image["image"]
                image_ext = base_image["ext"]

                img_filename = f"img_{page_num+1:03d}_{img_idx+1:03d}.{image_ext}"
                img_path = os.path.join(self.output_dir, "images", img_filename)

                with open(img_path, "wb") as f:
                    f.write(image_bytes)

                img_rects = page.get_image_rects(xref)
                bbox = {'x': 0, 'y': 0, 'width': 0, 'height': 0}

                if img_rects:
                    rect = img_rects[0]
                    bbox = {
                        'x': int(rect.x0),
                        'y': int(rect.y0),
                        'width': int(rect.width),
                        'height': int(rect.height)
                    }

                images.append({
                    'id': f'img_{page_num+1:03d}_{img_idx+1:03d}',
                    'filename': img_filename,
                    'path': img_path,
                    'page': page_num + 1,
                    'format': image_ext.upper(),
                    'bbox': bbox
                })

        doc.close()
        return images

    # -------------
    # Process PDF
    # -------------
    def process_pdf(self, pdf_path: str, debug: bool = False) -> Dict[str, Any]:
        print(f"\n{'='*80}")
        print(f"ğŸ“„ ÄANG Xá»¬ LÃ: {os.path.basename(pdf_path)}")
        print(f"{'='*80}\n")

        print("ğŸ–¼ï¸  BÆ¯á»šC 1: TrÃ­ch xuáº¥t hÃ¬nh áº£nh...")
        self.results['images'] = self.extract_images_from_pdf(pdf_path)
        print(f"âœ… ÄÃ£ trÃ­ch xuáº¥t {len(self.results['images'])} hÃ¬nh áº£nh\n")

        print("ğŸ” BÆ¯á»šC 2: OCR vÄƒn báº£n...")
        doc = fitz.open(pdf_path)

        all_text_parts: List[str] = []
        section_counter = 1
        total_pages = len(doc)

        for page_idx in range(total_pages):
            page_num = page_idx + 1
            print(f"\nğŸ“„ Trang {page_num}/{total_pages}")
            page = doc[page_idx]

            # Render page: tÄƒng zoom + sharpen nháº¹ báº±ng PIL
            zoom = 3.5
            mat = fitz.Matrix(zoom, zoom)
            pix = page.get_pixmap(matrix=mat, alpha=False)

            img = Image.frombytes("RGB", [pix.width, pix.height], pix.samples)
            # sharpen nháº¹
            img = ImageEnhance.Sharpness(img).enhance(1.5)
            img_array = np.array(img)

            layout_elements = self.process_page_image(img_array, page_num, debug=debug)

            current_section = ""

            for element in layout_elements:
                element['page'] = page_num

                if element['type'] == 'heading':
                    if current_section.strip():
                        self.results['sections'].append({
                            'id': f'section_{section_counter}',
                            'content': vi_cleanup(current_section) + '\n\n</break>\n',
                            'page': page_num
                        })
                        section_counter += 1
                        current_section = ""

                    current_section = f"\n{element['text']}\n\n"
                else:
                    current_section += element['text'] + " "

                all_text_parts.append(element['text'])

            if current_section.strip():
                self.results['sections'].append({
                    'id': f'section_{section_counter}',
                    'content': vi_cleanup(current_section),
                    'page': page_num
                })
                section_counter += 1

        doc.close()

        print(f"\nğŸ“ BÆ¯á»šC 3: Kiá»ƒm tra chÃ­nh táº£...")
        all_text = vi_cleanup(" ".join(all_text_parts))
        self.results['spelling_check'] = self.spell_checker.check_text(all_text)

        self.results['metadata'] = {
            'total_pages': total_pages,
            'total_sections': len(self.results['sections']),
            'total_images': len(self.results['images']),
            'total_words': self.results['spelling_check']['total_words'],
            'spelling_errors': len(self.results['spelling_check']['errors'])
        }

        print(f"\n{'='*80}")
        print("âœ… HOÃ€N Táº¤T Xá»¬ LÃ")
        print(f"{'='*80}")
        print(f"ğŸ“Š Tá»•ng sá»‘ trang: {self.results['metadata']['total_pages']}")
        print(f"ğŸ“ Tá»•ng sá»‘ pháº§n: {self.results['metadata']['total_sections']}")
        print(f"ğŸ–¼ï¸  Tá»•ng sá»‘ hÃ¬nh: {self.results['metadata']['total_images']}")
        print(f"ğŸ“– Tá»•ng sá»‘ tá»«: {self.results['metadata']['total_words']}")

        return self.results

    # ----------------
    # Export to Word
    # ----------------
    def export_to_word(self, output_filename: str = 'output_document.docx') -> str:
        print(f"\nğŸ’¾ Äang xuáº¥t file Word: {output_filename}")

        doc = Document()
        style = doc.styles['Normal']
        style.font.name = 'Times New Roman'
        style.font.size = Pt(13)

        # âœ… set font Ä‘á»§ Ä‘á»ƒ Word khÃ´ng rá»›t tiáº¿ng Viá»‡t
        rFonts = style._element.rPr.rFonts
        rFonts.set(qn('w:ascii'), 'Times New Roman')
        rFonts.set(qn('w:hAnsi'), 'Times New Roman')
        rFonts.set(qn('w:eastAsia'), 'Times New Roman')
        rFonts.set(qn('w:cs'), 'Times New Roman')

        title = doc.add_heading('TÃ€I LIá»†U ÄÃƒ Xá»¬ LÃ', 0)
        title.alignment = WD_ALIGN_PARAGRAPH.CENTER

        doc.add_heading('ThÃ´ng tin tá»•ng quan', 1)
        md = self.results.get('metadata', {})
        metadata_text = (
            f"Tá»•ng sá»‘ trang: {md.get('total_pages','N/A')}\n"
            f"Tá»•ng sá»‘ pháº§n: {md.get('total_sections','N/A')}\n"
            f"Tá»•ng sá»‘ hÃ¬nh áº£nh: {md.get('total_images','N/A')}\n"
            f"Tá»•ng sá»‘ tá»«: {md.get('total_words','N/A')}\n"
            f"Lá»—i chÃ­nh táº£: {md.get('spelling_errors','N/A')}\n"
        )
        doc.add_paragraph(metadata_text)

        doc.add_heading('Ná»™i dung', 1)

        for section in self.results.get('sections', []):
            content = section.get('content', '')
            content = vi_cleanup(content)

            if re.match(r'^\d+\.\d+', content.strip()):
                lines = content.split('\n', 1)
                heading_text = lines[0].strip()
                doc.add_heading(heading_text, 2)

                if len(lines) > 1:
                    body_text = lines[1].replace('</break>', '').strip()
                    body_text = vi_cleanup(body_text)
                    if body_text:
                        p = doc.add_paragraph(body_text)
                        p.alignment = WD_ALIGN_PARAGRAPH.JUSTIFY
            else:
                text = content.replace('</break>', '').strip()
                text = vi_cleanup(text)
                if text:
                    p = doc.add_paragraph(text)
                    p.alignment = WD_ALIGN_PARAGRAPH.JUSTIFY

            if '</break>' in content:
                doc.add_page_break()

        if self.results.get('images'):
            doc.add_page_break()
            doc.add_heading('Danh sÃ¡ch hÃ¬nh áº£nh', 1)

            for img in self.results['images']:
                doc.add_heading(f"HÃ¬nh {img['id']}", 2)
                try:
                    doc.add_picture(img['path'], width=Inches(4))
                except Exception:
                    doc.add_paragraph(f"[KhÃ´ng thá»ƒ thÃªm hÃ¬nh áº£nh: {img['filename']}]")

        output_path = os.path.join(self.output_dir, output_filename)
        doc.save(output_path)
        print(f"âœ… ÄÃ£ lÆ°u file Word: {output_path}\n")
        return output_path


def main():
    pdf_path = "input_document.pdf"
    if not os.path.exists(pdf_path):
        print(f"âŒ KhÃ´ng tÃ¬m tháº¥y file: {pdf_path}")
        print("ğŸ“ Äáº·t file PDF cáº§n xá»­ lÃ½ vÃ o thÆ° má»¥c hiá»‡n táº¡i vÃ  Ä‘á»•i tÃªn thÃ nh 'input_document.pdf'")
        return

    processor = EnhancedDocumentProcessor(output_dir='output')
    results = processor.process_pdf(pdf_path, debug=True)
    processor.export_to_word('tai_lieu_da_xu_ly.docx')


if __name__ == "__main__":
    main()
