"""
Enhanced Document Processor vá»›i OCR cáº£i tiáº¿n
- TÄƒng Ä‘á»™ phÃ¢n giáº£i cao hÆ¡n
- Preprocessing image tá»‘t hÆ¡n
- Xá»­ lÃ½ scan PDF hiá»‡u quáº£
"""

import os
import cv2
import fitz
import numpy as np
from PIL import Image, ImageEnhance
from paddleocr import PaddleOCR
from docx import Document
from docx.shared import Inches, Pt, RGBColor
from docx.enum.text import WD_ALIGN_PARAGRAPH
from pathlib import Path
import re
from typing import List, Dict, Tuple


class ImagePreprocessor:
    """Tiá»n xá»­ lÃ½ hÃ¬nh áº£nh trÆ°á»›c khi OCR"""
    
    @staticmethod
    def enhance_image(image: np.ndarray) -> np.ndarray:
        """Cáº£i thiá»‡n cháº¥t lÆ°á»£ng hÃ¬nh áº£nh"""
        
        # Chuyá»ƒn sang grayscale
        if len(image.shape) == 3:
            gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
        else:
            gray = image
        
        # 1. Loáº¡i bá» noise
        denoised = cv2.fastNlMeansDenoising(gray, None, 10, 7, 21)
        
        # 2. TÄƒng contrast (CLAHE - Contrast Limited Adaptive Histogram Equalization)
        clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))
        contrast = clahe.apply(denoised)
        
        # 3. Thresholding - chuyá»ƒn sang binary
        # Adaptive threshold tá»‘t cho vÄƒn báº£n scan
        binary = cv2.adaptiveThreshold(
            contrast, 255, 
            cv2.ADAPTIVE_THRESH_GAUSSIAN_C, 
            cv2.THRESH_BINARY, 
            11, 2
        )
        
        # 4. Morphological operations - lÃ m sáº¡ch
        kernel = np.ones((1,1), np.uint8)
        cleaned = cv2.morphologyEx(binary, cv2.MORPH_CLOSE, kernel)
        
        return cleaned
    
    @staticmethod
    def deskew_image(image: np.ndarray) -> np.ndarray:
        """Xoay tháº³ng hÃ¬nh áº£nh bá»‹ nghiÃªng"""
        try:
            coords = np.column_stack(np.where(image > 0))
            
            # Náº¿u khÃ´ng cÃ³ Ä‘iá»ƒm nÃ o (áº£nh toÃ n Ä‘en/tráº¯ng), return nguyÃªn
            if coords.shape[0] == 0:
                return image
            
            angle = cv2.minAreaRect(coords)[-1]
            
            if angle < -45:
                angle = -(90 + angle)
            else:
                angle = -angle
            
            if abs(angle) < 0.5:  # KhÃ´ng cáº§n xoay náº¿u gáº§n nhÆ° tháº³ng
                return image
            
            (h, w) = image.shape[:2]
            center = (w // 2, h // 2)
            M = cv2.getRotationMatrix2D(center, angle, 1.0)
            rotated = cv2.warpAffine(
                image, M, (w, h),
                flags=cv2.INTER_CUBIC,
                borderMode=cv2.BORDER_REPLICATE
            )
            
            return rotated
        except Exception as e:
            # Náº¿u lá»—i, return áº£nh gá»‘c
            return image
    
    @staticmethod
    def resize_for_ocr(image: np.ndarray, target_height=2000) -> np.ndarray:
        """Resize image vá» kÃ­ch thÆ°á»›c tá»‘i Æ°u cho OCR"""
        h, w = image.shape[:2]
        
        if h < target_height:
            # Scale up náº¿u quÃ¡ nhá»
            scale = target_height / h
            new_w = int(w * scale)
            new_h = target_height
            resized = cv2.resize(image, (new_w, new_h), interpolation=cv2.INTER_CUBIC)
            return resized
        elif h > target_height * 1.5:
            # Scale down náº¿u quÃ¡ lá»›n
            scale = target_height / h
            new_w = int(w * scale)
            new_h = target_height
            resized = cv2.resize(image, (new_w, new_h), interpolation=cv2.INTER_AREA)
            return resized
        
        return image


class VietnameseSpellChecker:
    """Kiá»ƒm tra chÃ­nh táº£ tiáº¿ng Viá»‡t"""
    
    def __init__(self):
        self.dictionary = set([
            'vÃ ', 'cá»§a', 'cÃ³', 'trong', 'lÃ ', 'Ä‘Æ°á»£c', 'vá»›i', 'cÃ¡c', 'cho', 'Ä‘á»ƒ',
            'Ä‘Ã£', 'nÃ y', 'theo', 'nhá»¯ng', 'ngÆ°á»i', 'tá»«', 'má»™t', 'nÄƒm', 'khi', 'vá»',
            'ná»™i', 'dung', 'hÃ¬nh', 'áº£nh', 'Ä‘á»“', 'thá»‹', 'tÃ i', 'liá»‡u', 'chÆ°Æ¡ng', 'má»¥c',
            'pháº§n', 'báº£ng', 'sá»‘', 'liá»‡u', 'thÃ´ng', 'tin', 'dá»¯', 'kiá»ƒm', 'tra', 'xá»­', 'lÃ½',
            'phÆ°Æ¡ng', 'phÃ¡p', 'káº¿t', 'quáº£', 'nghiÃªn', 'cá»©u', 'phÃ¡t', 'triá»ƒn', 'há»‡', 'thá»‘ng',
        ])
        self.load_extended_dictionary()
    
    def load_extended_dictionary(self, dict_file='vietnamese_dict.txt'):
        if os.path.exists(dict_file):
            with open(dict_file, 'r', encoding='utf-8') as f:
                words = f.read().splitlines()
                self.dictionary.update(words)
    
    def check_text(self, text: str) -> Dict:
        words = re.findall(r'[\wÃ Ã¡áº¡áº£Ã£Ã¢áº§áº¥áº­áº©áº«Äƒáº±áº¯áº·áº³áºµÃ¨Ã©áº¹áº»áº½Ãªá»áº¿á»‡á»ƒá»…Ã¬Ã­á»‹á»‰Ä©Ã²Ã³á»á»ÃµÃ´á»“á»‘á»™á»•á»—Æ¡á»á»›á»£á»Ÿá»¡Ã¹Ãºá»¥á»§Å©Æ°á»«á»©á»±á»­á»¯á»³Ã½á»µá»·á»¹Ä‘]+', text.lower())
        errors = []
        
        for idx, word in enumerate(words):
            if len(word) > 2 and word not in self.dictionary:
                errors.append({'word': word, 'position': idx, 'suggestions': []})
        
        return {
            'total_words': len(words),
            'errors': errors,
            'error_rate': len(errors) / len(words) if len(words) > 0 else 0
        }


class EnhancedDocumentProcessor:
    """Document processor vá»›i OCR cáº£i tiáº¿n"""
    
    def __init__(self, output_dir='output'):
        print("ğŸ”§ Äang khá»Ÿi táº¡o PaddleOCR...")
        print("   (Láº§n Ä‘áº§u sáº½ táº£i model ~100MB, chá» vÃ i phÃºt...)")
        
        # Khá»Ÿi táº¡o OCR vá»›i config má»›i
        self.ocr = PaddleOCR(use_textline_orientation=True, lang='vi')
        
        self.preprocessor = ImagePreprocessor()
        self.spell_checker = VietnameseSpellChecker()
        self.output_dir = output_dir
        
        os.makedirs(output_dir, exist_ok=True)
        os.makedirs(f"{output_dir}/images", exist_ok=True)
        os.makedirs(f"{output_dir}/debug", exist_ok=True)
        
        self.results = {
            'sections': [],
            'images': [],
            'spelling_check': {},
            'metadata': {}
        }
        
        print("âœ… PaddleOCR sáºµn sÃ ng!\n")
    
    def process_page_image(self, image: np.ndarray, page_num: int, debug=False) -> List[Dict]:
        """Xá»­ lÃ½ má»™t trang - Bá» preprocessing Ä‘á»ƒ trÃ¡nh lá»—i"""
        
        print(f"   ğŸ” Äang OCR trang {page_num}...")
        
        # OCR trá»±c tiáº¿p - KHÃ”NG preprocessing
        try:
            ocr_result = self.ocr.predict(image)  # DÃ¹ng áº£nh gá»‘c luÃ´n
        except Exception as e:
            print(f"   âŒ Lá»—i OCR: {e}")
            return []
        
        layout_elements = []
        
        if not ocr_result or len(ocr_result) == 0:
            print(f"   âš ï¸  KhÃ´ng phÃ¡t hiá»‡n text trong trang {page_num}")
            return layout_elements
        
        # Parse káº¿t quáº£ má»›i - result lÃ  list of dict
        text_count = 0
        result_dict = ocr_result[0]  # Láº¥y dict Ä‘áº§u tiÃªn
        
        # Láº¥y dá»¯ liá»‡u tá»« dict
        texts = result_dict.get('rec_texts', [])
        scores = result_dict.get('rec_scores', [])
        polys = result_dict.get('rec_polys', [])
        
        print(f"   ğŸ“Š PhÃ¡t hiá»‡n {len(texts)} dÃ²ng text")
        
        for i in range(len(texts)):
            try:
                text = texts[i]
                confidence = scores[i] if i < len(scores) else 1.0
                poly = polys[i] if i < len(polys) else None
                
                # Chá»‰ láº¥y text cÃ³ confidence > 0.5
                if confidence < 0.5:
                    continue
                
                text_count += 1
                
                # Xá»­ lÃ½ bounding box
                if poly is not None and len(poly) >= 4:
                    # poly lÃ  numpy array shape (4, 2): [[x1,y1], [x2,y2], [x3,y3], [x4,y4]]
                    x_coords = [float(p[0]) for p in poly]
                    y_coords = [float(p[1]) for p in poly]
                    
                    x_min, x_max = min(x_coords), max(x_coords)
                    y_min, y_max = min(y_coords), max(y_coords)
                    
                    width = x_max - x_min
                    height = y_max - y_min
                else:
                    x_min, y_min, width, height = 0, 0, 100, 20
                
                element_type = self.classify_element(text, width, height, y_min)
                
                layout_elements.append({
                    'type': element_type,
                    'bbox': {
                        'x': int(x_min),
                        'y': int(y_min),
                        'width': int(width),
                        'height': int(height)
                    },
                    'text': text,
                    'confidence': float(confidence)
                })
                
            except Exception as e:
                print(f"   âš ï¸  Bá» qua dÃ²ng lá»—i: {e}")
                continue
        
        print(f"   âœ… PhÃ¡t hiá»‡n {text_count} dÃ²ng text (confidence > 0.5)")
        return layout_elements
    
    def classify_element(self, text: str, width: float, height: float, y_pos: float) -> str:
        """PhÃ¢n loáº¡i element"""
        if height > 30 and y_pos < 200:
            return 'title'
        if re.match(r'^\d+\.\d+', text.strip()):
            return 'heading'
        return 'text'
    
    def extract_images_from_pdf(self, pdf_path: str) -> List[Dict]:
        """TrÃ­ch xuáº¥t hÃ¬nh áº£nh tá»« PDF"""
        doc = fitz.open(pdf_path)
        images = []
        
        for page_num in range(len(doc)):
            page = doc[page_num]
            image_list = page.get_images()
            
            for img_idx, img in enumerate(image_list):
                xref = img[0]
                base_image = doc.extract_image(xref)
                image_bytes = base_image["image"]
                image_ext = base_image["ext"]
                
                img_filename = f"img_{page_num+1:03d}_{img_idx+1:03d}.{image_ext}"
                img_path = os.path.join(self.output_dir, "images", img_filename)
                
                with open(img_path, "wb") as f:
                    f.write(image_bytes)
                
                img_rects = page.get_image_rects(xref)
                bbox = {'x': 0, 'y': 0, 'width': 0, 'height': 0}
                
                if img_rects:
                    rect = img_rects[0]
                    bbox = {
                        'x': int(rect.x0),
                        'y': int(rect.y0),
                        'width': int(rect.width),
                        'height': int(rect.height)
                    }
                
                images.append({
                    'id': f'img_{page_num+1:03d}_{img_idx+1:03d}',
                    'filename': img_filename,
                    'path': img_path,
                    'page': page_num + 1,
                    'format': image_ext.upper(),
                    'bbox': bbox
                })
        
        doc.close()
        return images
    
    def process_pdf(self, pdf_path: str, debug=False) -> Dict:
        """Xá»­ lÃ½ PDF vá»›i OCR cáº£i tiáº¿n"""
        print(f"\n{'='*80}")
        print(f"ğŸ“„ ÄANG Xá»¬ LÃ: {os.path.basename(pdf_path)}")
        print(f"{'='*80}\n")
        
        # TrÃ­ch xuáº¥t hÃ¬nh áº£nh
        print("ğŸ–¼ï¸  BÆ¯á»šC 1: TrÃ­ch xuáº¥t hÃ¬nh áº£nh...")
        self.results['images'] = self.extract_images_from_pdf(pdf_path)
        print(f"âœ… ÄÃ£ trÃ­ch xuáº¥t {len(self.results['images'])} hÃ¬nh áº£nh\n")
        
        # OCR
        print("ğŸ” BÆ¯á»šC 2: OCR vÄƒn báº£n...")
        doc = fitz.open(pdf_path)
        
        all_text = ""
        section_counter = 1
        total_pages = len(doc)  # LÆ°u sá»‘ trang TRÆ¯á»šC khi close
        
        for page_num in range(total_pages):
            print(f"\nğŸ“„ Trang {page_num + 1}/{total_pages}")
            
            page = doc[page_num]
            
            # Chuyá»ƒn page sang image vá»›i Ä‘á»™ phÃ¢n giáº£i cao
            zoom = 3  # TÄƒng Ä‘á»™ phÃ¢n giáº£i lÃªn 3x
            mat = fitz.Matrix(zoom, zoom)
            pix = page.get_pixmap(matrix=mat)
            
            # Chuyá»ƒn sang numpy array
            img = Image.frombytes("RGB", [pix.width, pix.height], pix.samples)
            img_array = np.array(img)
            
            # Process vá»›i preprocessing
            layout_elements = self.process_page_image(img_array, page_num + 1, debug=debug)
            
            # Xá»­ lÃ½ káº¿t quáº£
            current_section = ""
            
            for element in layout_elements:
                element['page'] = page_num + 1
                
                if element['type'] == 'heading':
                    if current_section:
                        self.results['sections'].append({
                            'id': f'section_{section_counter}',
                            'content': current_section.strip() + '\n\n</break>\n',
                            'page': page_num + 1
                        })
                        section_counter += 1
                        current_section = ""
                    
                    current_section = f"\n{element['text']}\n\n"
                else:
                    current_section += element['text'] + " "
                
                all_text += element['text'] + " "
            
            if current_section:
                self.results['sections'].append({
                    'id': f'section_{section_counter}',
                    'content': current_section.strip(),
                    'page': page_num + 1
                })
                section_counter += 1
        
        doc.close()
        
        # Kiá»ƒm tra chÃ­nh táº£
        print(f"\nğŸ“ BÆ¯á»šC 3: Kiá»ƒm tra chÃ­nh táº£...")
        self.results['spelling_check'] = self.spell_checker.check_text(all_text)
        
        # Metadata (dÃ¹ng total_pages Ä‘Ã£ lÆ°u)
        self.results['metadata'] = {
            'total_pages': total_pages,
            'total_sections': len(self.results['sections']),
            'total_images': len(self.results['images']),
            'total_words': self.results['spelling_check']['total_words'],
            'spelling_errors': len(self.results['spelling_check']['errors'])
        }
        
        print(f"\n{'='*80}")
        print("âœ… HOÃ€N Táº¤T Xá»¬ LÃ")
        print(f"{'='*80}")
        print(f"ğŸ“Š Tá»•ng sá»‘ trang: {self.results['metadata']['total_pages']}")
        print(f"ğŸ“ Tá»•ng sá»‘ pháº§n: {self.results['metadata']['total_sections']}")
        print(f"ğŸ–¼ï¸  Tá»•ng sá»‘ hÃ¬nh: {self.results['metadata']['total_images']}")
        print(f"ğŸ“– Tá»•ng sá»‘ tá»«: {self.results['metadata']['total_words']}")
        
        return self.results
    
    def export_to_word(self, output_filename='output_document.docx'):
        """Xuáº¥t Word"""
        print(f"\nğŸ’¾ Äang xuáº¥t file Word: {output_filename}")
        
        doc = Document()
        style = doc.styles['Normal']
        style.font.name = 'Times New Roman'
        style.font.size = Pt(13)
        
        title = doc.add_heading('TÃ€I LIá»†U ÄÃƒ Xá»¬ LÃ', 0)
        title.alignment = WD_ALIGN_PARAGRAPH.CENTER
        
        doc.add_heading('ThÃ´ng tin tá»•ng quan', 1)
        metadata_text = f"""
Tá»•ng sá»‘ trang: {self.results['metadata']['total_pages']}
Tá»•ng sá»‘ pháº§n: {self.results['metadata']['total_sections']}
Tá»•ng sá»‘ hÃ¬nh áº£nh: {self.results['metadata']['total_images']}
Tá»•ng sá»‘ tá»«: {self.results['metadata']['total_words']}
Lá»—i chÃ­nh táº£: {self.results['metadata']['spelling_errors']}
        """
        doc.add_paragraph(metadata_text)
        
        doc.add_heading('Ná»™i dung', 1)
        
        for section in self.results['sections']:
            content = section['content']
            
            if re.match(r'^\d+\.\d+', content.strip()):
                lines = content.split('\n', 1)
                heading_text = lines[0].strip()
                doc.add_heading(heading_text, 2)
                
                if len(lines) > 1:
                    body_text = lines[1].replace('</break>', '').strip()
                    if body_text:
                        p = doc.add_paragraph(body_text)
                        p.alignment = WD_ALIGN_PARAGRAPH.JUSTIFY
            else:
                text = content.replace('</break>', '').strip()
                if text:
                    p = doc.add_paragraph(text)
                    p.alignment = WD_ALIGN_PARAGRAPH.JUSTIFY
            
            if '</break>' in content:
                doc.add_page_break()
        
        if self.results['images']:
            doc.add_page_break()
            doc.add_heading('Danh sÃ¡ch hÃ¬nh áº£nh', 1)
            
            for img in self.results['images']:
                doc.add_heading(f"HÃ¬nh {img['id']}", 2)
                try:
                    doc.add_picture(img['path'], width=Inches(4))
                except:
                    doc.add_paragraph(f"[KhÃ´ng thá»ƒ thÃªm hÃ¬nh áº£nh: {img['filename']}]")
        
        output_path = os.path.join(self.output_dir, output_filename)
        doc.save(output_path)
        print(f"âœ… ÄÃ£ lÆ°u file Word: {output_path}\n")
        
        return output_path


def main():
    pdf_path = "input_document.pdf"
    
    if not os.path.exists(pdf_path):
        print(f"âŒ KhÃ´ng tÃ¬m tháº¥y file: {pdf_path}")
        print("ğŸ“ Äáº·t file PDF cáº§n xá»­ lÃ½ vÃ o thÆ° má»¥c hiá»‡n táº¡i vÃ  Ä‘á»•i tÃªn thÃ nh 'input_document.pdf'")
        return
    
    processor = EnhancedDocumentProcessor(output_dir='output')
    results = processor.process_pdf(pdf_path, debug=True)  # debug=True Ä‘á»ƒ xem áº£nh Ä‘Ã£ xá»­ lÃ½
    processor.export_to_word('tai_lieu_da_xu_ly.docx')


if __name__ == "__main__":
    main()